{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import pickle\n",
    "from catboost import CatBoostClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn import tree\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    '''A simple AdaBoost Classifier.'''\n",
    "\n",
    "    def __init__(self, n_weakers_limit):\n",
    "        self.weights = []\n",
    "        self.classifiers = []\n",
    "        self.n_weakers_limit = n_weakers_limit\n",
    "        self.ws = []\n",
    "        pass\n",
    "\n",
    "    def is_good_enough(self,X_train,y_train):\n",
    "        #if((len(self.classifiers) > self.n_weakers_limit) or\n",
    "        #        accuracy_score(y_train, self.predict(X_train))>=0.9):\n",
    "        if(len(self.classifiers) > self.n_weakers_limit):\n",
    "            C_report = classification_report(y_train, self.predict(X_train), digits=4, output_dict=True)\n",
    "            print(classification_report(y_train, self.predict(X_train), digits=4))\n",
    "            report = f1_score(y_train, self.predict(X_train), average=None)\n",
    "            print('Score: ', report)\n",
    "            df = pd.DataFrame(C_report).transpose()\n",
    "            df.to_csv(\"classifier_report.csv\", index= True)\n",
    "            return True\n",
    "        else:\n",
    "            # print(\"准确率：\" + str(accuracy_score(y_train, self.predict(X_train))))\n",
    "            print(classification_report(y_train, self.predict(X_train), digits=4))\n",
    "            report = f1_score(y_train, self.predict(X_train), average=None)\n",
    "            print('Score: ', report)\n",
    "            return False\n",
    "        pass\n",
    "\n",
    "    def fit(self,X_train,y_train):\n",
    "        w = np.ones((1, len(X_train))) / len(y_train)\n",
    "        flag = 0\n",
    "        while flag == 0 or (not (self.is_good_enough(X_train, y_train))):\n",
    "            # for i in range(2):\n",
    "            # 定义并训练基分类器决策树\n",
    "            flag = 1\n",
    "            clf = tree.DecisionTreeClassifier \\\n",
    "                (class_weight=\"balanced\", criterion='gini', max_depth=1, splitter=\"random\", random_state=0)\n",
    "            predit_tree = clf.fit(X_train, y_train, w.reshape(-1))\n",
    "            # predit_tree = clf.fit(X_train, y_train)\n",
    "            self.classifiers.append(predit_tree)\n",
    "            # 计算错误率\n",
    "            correct_rate = predit_tree.score(X_train, y_train)\n",
    "            error_rate = 1 - correct_rate\n",
    "            # 计算分类器权重\n",
    "            alpha = 1 / 2 * np.log((1 - error_rate) / error_rate)\n",
    "            # 存储分类器权重\n",
    "            self.weights.append(alpha)\n",
    "            # 计算预测结果\n",
    "            result = predit_tree.predict(X_train).reshape(-1, 1)\n",
    "            # 更新数据集权重参数\n",
    "            first_part = -alpha * y_train\n",
    "            second_part = np.exp(first_part * result)\n",
    "            third_part = (w.reshape(-1, 1) * second_part)\n",
    "            z_t = np.sum(third_part)\n",
    "            w = w.reshape(-1, 1) * second_part / z_t\n",
    "            self.ws.append(w)\n",
    "            print(\"alpha=\", alpha)\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict_scores(self, X_train):\n",
    "        '''Calculate the weighted sum score of the whole base classifiers for given samples.\n",
    "\n",
    "        Args:\n",
    "            X: An ndarray indicating the samples to be predicted, which shape should be (n_samples,n_features).\n",
    "\n",
    "        Returns:\n",
    "            An one-dimension ndarray indicating the scores of differnt samples, which shape should be (n_samples,1).\n",
    "        '''\n",
    "        scores = np.zeros((1, len(X_train)))\n",
    "        for i in range(len(self.classifiers)):\n",
    "            scores +=  self.classifiers[i].predict(X_train)*self.weights[i]    \n",
    "            \n",
    "        return scores\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_train, threshold=0):\n",
    "        '''Predict the catagories for geven samples.\n",
    "\n",
    "        Args:\n",
    "            X: An ndarray indicating the samples to be predicted, which shape should be (n_samples,n_features).\n",
    "            threshold: The demarcation number of deviding the samples into two parts.\n",
    "\n",
    "        Returns:\n",
    "            An ndarray consists of predicted labels, which shape should be (n_samples,1).\n",
    "        '''\n",
    "        scores = self.predict_scores(X_train)\n",
    "        y_train = np.where(scores>threshold, 1, -1)\n",
    "        return y_train.reshape(-1,1)\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def save(model, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('is_train_data/is_train_20190701.txt')\n",
    "train = pd.merge(train[['link','current_slice_id','future_slice_id']][(train.label == 2)],train,how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    train1 = pd.read_csv('is_train_data/is_train_2019070'+str(i)+'.txt')\n",
    "    train = train.append(pd.merge(train1[['link','current_slice_id','future_slice_id']][(train1.label == 2)],train1,how='left'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>current_slice_id</th>\n",
       "      <th>future_slice_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>now_speed</th>\n",
       "      <th>now_eta</th>\n",
       "      <th>now_cnt</th>\n",
       "      <th>now_state</th>\n",
       "      <th>current_speed_min</th>\n",
       "      <th>...</th>\n",
       "      <th>his_7_eta_mean</th>\n",
       "      <th>his_7_eta_std</th>\n",
       "      <th>his_7_cnt_min</th>\n",
       "      <th>his_7_cnt_max</th>\n",
       "      <th>his_7_cnt_mean</th>\n",
       "      <th>his_7_cnt_std</th>\n",
       "      <th>his_7_state_zhong</th>\n",
       "      <th>his_7_state_max</th>\n",
       "      <th>his_7_state_min</th>\n",
       "      <th>is_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33503</td>\n",
       "      <td>409</td>\n",
       "      <td>437</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>36.5</td>\n",
       "      <td>34.1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>35.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>16.656250</td>\n",
       "      <td>3.148438</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.743560</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>277708</td>\n",
       "      <td>536</td>\n",
       "      <td>547</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>24.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3.599609</td>\n",
       "      <td>...</td>\n",
       "      <td>8.296875</td>\n",
       "      <td>0.877930</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.788854</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34756</td>\n",
       "      <td>539</td>\n",
       "      <td>560</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.359375</td>\n",
       "      <td>1.260742</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>24.4</td>\n",
       "      <td>3.136877</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>273215</td>\n",
       "      <td>200</td>\n",
       "      <td>221</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>55.2</td>\n",
       "      <td>36.7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>52.312500</td>\n",
       "      <td>...</td>\n",
       "      <td>14.054688</td>\n",
       "      <td>1.059570</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>18.4</td>\n",
       "      <td>2.154066</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639183</td>\n",
       "      <td>241</td>\n",
       "      <td>247</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>5.898438</td>\n",
       "      <td>...</td>\n",
       "      <td>16.625000</td>\n",
       "      <td>2.701172</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>32.4</td>\n",
       "      <td>5.535341</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22189</th>\n",
       "      <td>171823</td>\n",
       "      <td>337</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>16.6</td>\n",
       "      <td>14.8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.796875</td>\n",
       "      <td>0.363525</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22190</th>\n",
       "      <td>209839</td>\n",
       "      <td>589</td>\n",
       "      <td>612</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3.099609</td>\n",
       "      <td>...</td>\n",
       "      <td>24.421875</td>\n",
       "      <td>0.830078</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>11.4</td>\n",
       "      <td>2.939388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22191</th>\n",
       "      <td>370809</td>\n",
       "      <td>604</td>\n",
       "      <td>607</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.699219</td>\n",
       "      <td>...</td>\n",
       "      <td>22.015625</td>\n",
       "      <td>1.762695</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.748331</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22192</th>\n",
       "      <td>162241</td>\n",
       "      <td>678</td>\n",
       "      <td>680</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>28.3</td>\n",
       "      <td>26.8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>28.296875</td>\n",
       "      <td>...</td>\n",
       "      <td>26.031250</td>\n",
       "      <td>4.699219</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.356466</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22193</th>\n",
       "      <td>300798</td>\n",
       "      <td>531</td>\n",
       "      <td>534</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6.601562</td>\n",
       "      <td>...</td>\n",
       "      <td>30.953125</td>\n",
       "      <td>2.027344</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196323 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         link  current_slice_id  future_slice_id  label  time_diff  now_speed  \\\n",
       "0       33503               409              437      2         28       36.5   \n",
       "1      277708               536              547      2         11       24.3   \n",
       "2       34756               539              560      2         21        6.0   \n",
       "3      273215               200              221      2         21       55.2   \n",
       "4      639183               241              247      2          6        6.5   \n",
       "...       ...               ...              ...    ...        ...        ...   \n",
       "22189  171823               337              352      2         15       16.6   \n",
       "22190  209839               589              612      2         23        3.1   \n",
       "22191  370809               604              607      2          3        2.9   \n",
       "22192  162241               678              680      2          2       28.3   \n",
       "22193  300798               531              534      2          3       11.2   \n",
       "\n",
       "       now_eta  now_cnt  now_state  current_speed_min  ...  his_7_eta_mean  \\\n",
       "0         34.1        5          1          35.187500  ...       16.656250   \n",
       "1          7.4       20          2           3.599609  ...        8.296875   \n",
       "2         12.9       19          3           5.000000  ...       13.359375   \n",
       "3         36.7        7          1          52.312500  ...       14.054688   \n",
       "4         12.5       42          3           5.898438  ...       16.625000   \n",
       "...        ...      ...        ...                ...  ...             ...   \n",
       "22189     14.8        9          2          16.000000  ...        9.796875   \n",
       "22190      1.9        6          4           3.099609  ...       24.421875   \n",
       "22191      3.0        7          4           2.699219  ...       22.015625   \n",
       "22192     26.8        9          1          28.296875  ...       26.031250   \n",
       "22193      5.5        8          3           6.601562  ...       30.953125   \n",
       "\n",
       "       his_7_eta_std  his_7_cnt_min  his_7_cnt_max  his_7_cnt_mean  \\\n",
       "0           3.148438              3              8             4.6   \n",
       "1           0.877930             27             32            29.0   \n",
       "2           1.260742             20             28            24.4   \n",
       "3           1.059570             15             21            18.4   \n",
       "4           2.701172             25             40            32.4   \n",
       "...              ...            ...            ...             ...   \n",
       "22189       0.363525              6              8             7.4   \n",
       "22190       0.830078              7             16            11.4   \n",
       "22191       1.762695             11             13            12.2   \n",
       "22192       4.699219              1              4             2.4   \n",
       "22193       2.027344              5              7             6.4   \n",
       "\n",
       "       his_7_cnt_std  his_7_state_zhong  his_7_state_max  his_7_state_min  \\\n",
       "0           1.743560                  2                2                2   \n",
       "1           1.788854                  3                4                3   \n",
       "2           3.136877                  3                4                3   \n",
       "3           2.154066                  2                3                1   \n",
       "4           5.535341                  2                3                2   \n",
       "...              ...                ...              ...              ...   \n",
       "22189       0.800000                  2                2                2   \n",
       "22190       2.939388                  1                1                1   \n",
       "22191       0.748331                  1                2                1   \n",
       "22192       1.356466                  1                1                1   \n",
       "22193       0.800000                  1                2                1   \n",
       "\n",
       "       is_high  \n",
       "0            0  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "22189        0  \n",
       "22190        0  \n",
       "22191        0  \n",
       "22192        0  \n",
       "22193        1  \n",
       "\n",
       "[196323 rows x 85 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(y == 2 , 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost = AdaBoostClassifier(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(train.loc[:,'time_diff':'width'],y,test_size=0.25 , random_state=1234)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train,y,test_size=0.2 , random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha= 0.5836402861116483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9885    0.7627    0.8611    390270\n",
      "           1     0.1064    0.7606    0.1866     14492\n",
      "\n",
      "    accuracy                         0.7627    404762\n",
      "   macro avg     0.5474    0.7617    0.5239    404762\n",
      "weighted avg     0.9569    0.7627    0.8369    404762\n",
      "\n",
      "Score:  [0.8610533  0.18664861]\n",
      "alpha= 0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9885    0.7627    0.8611    390270\n",
      "           1     0.1064    0.7606    0.1866     14492\n",
      "\n",
      "    accuracy                         0.7627    404762\n",
      "   macro avg     0.5474    0.7617    0.5239    404762\n",
      "weighted avg     0.9569    0.7627    0.8369    404762\n",
      "\n",
      "Score:  [0.8610533  0.18664861]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9885    0.7627    0.8611    390270\n",
      "           1     0.1064    0.7606    0.1866     14492\n",
      "\n",
      "    accuracy                         0.7627    404762\n",
      "   macro avg     0.5474    0.7617    0.5239    404762\n",
      "weighted avg     0.9569    0.7627    0.8369    404762\n",
      "\n",
      "Score:  [0.8610533  0.18664861]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n",
      "alpha= -0.9848892640318742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.9778    0.8933    0.9337    390270\n",
      "           1     0.1367    0.4549    0.2102     14492\n",
      "\n",
      "    accuracy                         0.8776    404762\n",
      "   macro avg     0.5572    0.6741    0.5719    404762\n",
      "weighted avg     0.9477    0.8776    0.9077    404762\n",
      "\n",
      "Score:  [0.93365221 0.21016387]\n"
     ]
    }
   ],
   "source": [
    "AdaBoost.fit(X_train, y_train.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
