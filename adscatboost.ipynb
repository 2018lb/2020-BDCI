{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import pickle\n",
    "from catboost import CatBoostClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn import tree\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    '''A simple AdaBoost Classifier.'''\n",
    "\n",
    "    def __init__(self, n_weakers_limit):\n",
    "        self.weights = []\n",
    "        self.classifiers = []\n",
    "        self.n_weakers_limit = n_weakers_limit\n",
    "        self.ws = []\n",
    "        pass\n",
    "\n",
    "    def is_good_enough(self,X_train,y_train):\n",
    "        #if((len(self.classifiers) > self.n_weakers_limit) or\n",
    "        #        accuracy_score(y_train, self.predict(X_train))>=0.9):\n",
    "        if(len(self.classifiers) > self.n_weakers_limit):\n",
    "            C_report = classification_report(y_train, self.predict(X_train), digits=4, output_dict=True)\n",
    "            print(classification_report(y_train, self.predict(X_train), digits=4))\n",
    "            report = f1_score(y_train, self.predict(X_train), average=None)\n",
    "            print('Score: ', report)\n",
    "            df = pd.DataFrame(C_report).transpose()\n",
    "            df.to_csv(\"classifier_report.csv\", index= True)\n",
    "            return True\n",
    "        else:\n",
    "            # print(\"准确率：\" + str(accuracy_score(y_train, self.predict(X_train))))\n",
    "            print(classification_report(y_train, self.predict(X_train), digits=4))\n",
    "            report = f1_score(y_train, self.predict(X_train), average=None)\n",
    "            print('Score: ', report)\n",
    "            return False\n",
    "        pass\n",
    "\n",
    "    def fit(self,X_train,y_train):\n",
    "        w = np.ones((1, len(X_train))) / len(y_train)\n",
    "        flag = 0\n",
    "        while flag == 0 or (not (self.is_good_enough(X_train, y_train))):\n",
    "            # for i in range(2):\n",
    "            # 定义并训练基分类器决策树\n",
    "            flag = 1\n",
    "            clf = tree.DecisionTreeClassifier \\\n",
    "                (class_weight=\"balanced\", criterion='gini', max_depth=1, splitter=\"random\", random_state=0)\n",
    "            predit_tree = clf.fit(X_train, y_train, w.reshape(-1))\n",
    "            # predit_tree = clf.fit(X_train, y_train)\n",
    "            self.classifiers.append(predit_tree)\n",
    "            # 计算错误率\n",
    "            correct_rate = predit_tree.score(X_train, y_train)\n",
    "            error_rate = 1 - correct_rate\n",
    "            # 计算分类器权重\n",
    "            alpha = 1 / 2 * np.log((1 - error_rate) / error_rate)\n",
    "            # 存储分类器权重\n",
    "            self.weights.append(alpha)\n",
    "            # 计算预测结果\n",
    "            result = predit_tree.predict(X_train).reshape(-1, 1)\n",
    "            # 更新数据集权重参数\n",
    "            first_part = -alpha * y_train\n",
    "            second_part = np.exp(first_part * result)\n",
    "            third_part = (w.reshape(-1, 1) * second_part)\n",
    "            z_t = np.sum(third_part)\n",
    "            w = w.reshape(-1, 1) * second_part / z_t\n",
    "            self.ws.append(w)\n",
    "            print(\"alpha=\", alpha)\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict_scores(self, X_train):\n",
    "        '''Calculate the weighted sum score of the whole base classifiers for given samples.\n",
    "\n",
    "        Args:\n",
    "            X: An ndarray indicating the samples to be predicted, which shape should be (n_samples,n_features).\n",
    "\n",
    "        Returns:\n",
    "            An one-dimension ndarray indicating the scores of differnt samples, which shape should be (n_samples,1).\n",
    "        '''\n",
    "        scores = np.zeros((1, len(X_train)))\n",
    "        for i in range(len(self.classifiers)):\n",
    "            scores +=  self.classifiers[i].predict(X_train)*self.weights[i]    \n",
    "            \n",
    "        return scores\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_train, threshold=0):\n",
    "        '''Predict the catagories for geven samples.\n",
    "\n",
    "        Args:\n",
    "            X: An ndarray indicating the samples to be predicted, which shape should be (n_samples,n_features).\n",
    "            threshold: The demarcation number of deviding the samples into two parts.\n",
    "\n",
    "        Returns:\n",
    "            An ndarray consists of predicted labels, which shape should be (n_samples,1).\n",
    "        '''\n",
    "        scores = self.predict_scores(X_train)\n",
    "        y_train = np.where(scores>threshold, 1, -1)\n",
    "        return y_train.reshape(-1,1)\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def save(model, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('is_train_data/is_train_20190701.txt')\n",
    "train = pd.merge(train[['link','current_slice_id','future_slice_id']][(train.label == 2)],train,how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    train1 = pd.read_csv('is_train_data/is_train_2019070'+str(i)+'.txt')\n",
    "    train = train.append(pd.merge(train1[['link','current_slice_id','future_slice_id']][(train1.label == 2)],train1,how='left'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(y == 2 , 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost = AdaBoostClassifier(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(train.loc[:,'time_diff':'width'],y,test_size=0.25 , random_state=1234)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train,y,test_size=0.2 , random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost.fit(X_train, y_train.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
