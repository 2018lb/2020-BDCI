{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import pickle\n",
    "from catboost import CatBoostClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = pd.read_csv('per_road_pro.txt')\n",
    "attr = pd.read_table('attr.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train/is_train_20190702.txt')\n",
    "train = train.join(attr.set_index('linkid'), on='link')\n",
    "train = pd.merge(train, pro, how='left')\n",
    "y = train.label\n",
    "train = train.drop('label',1)\n",
    "y = np.where(y == 2, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入验证机集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('train//is_train_20190730.txt')\n",
    "train_1 = train_1.join(attr.set_index('linkid'), on='link')\n",
    "y_5 = train_1.label\n",
    "train_1 = pd.merge(train_1, pro, how='left')\n",
    "train_1 = train_1.drop('label',1)\n",
    "y_5 = np.where(y_5 == 2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 待预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = pd.read_csv('one_day_speed.txt')\n",
    "T = T.join(attr.set_index('linkid'), on='link')\n",
    "T = pd.merge(T, pro, how='left')\n",
    "y_1 = T.label\n",
    "T = T.drop('label',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk1 = y_1\n",
    "kkk1 = np.where(kkk1 <500, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kkk1 = kkk1.reshape(-1,1)\n",
    "cnt_y = np.where(y_1 == 2, 1, 0)\n",
    "for i in tqdm(range(2,10)):\n",
    "    path = 'train/is_train_2019070'+str(i)+'.txt'\n",
    "    train = pd.read_csv(path)\n",
    "    train = train.join(attr.set_index('linkid'), on='link')\n",
    "    train = pd.merge(train, pro, how='left')\n",
    "    y = train.label\n",
    "    train = train.drop('label',1)\n",
    "    y = np.where(y == 2, 1, 0)\n",
    "    # class_weight={0:0.6,1:0.2}\n",
    "    w = np.array(1/(y+1.2))\n",
    "    model_cat = CatBoostClassifier(iterations=200,depth=6,l2_leaf_reg=50,learning_rate=0.8,random_strength=1,\n",
    "                                      loss_function='MultiClassOneVsAll',verbose=False)\n",
    "\n",
    "    model_cat.fit(train,y,eval_set=(train_1,y_5),early_stopping_rounds=20,sample_weight = w)\n",
    "    kkk = model_cat.predict(T)\n",
    "    kkk1 = np.where((kkk == 1),kkk1+1,kkk1)\n",
    "    print(classification_report(cnt_y, kkk, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(10,30)):\n",
    "    path = 'train/is_train_201907'+str(i)+'.txt'\n",
    "    train = pd.read_csv(path)\n",
    "    train = train.join(attr.set_index('linkid'), on='link')\n",
    "    train = pd.merge(train, pro, how='left')\n",
    "    y = train.label\n",
    "    train = train.drop('label',1)\n",
    "    y = np.where(y == 2, 1, 0)\n",
    "    # class_weight={0:0.6,1:0.2}\n",
    "    w = np.array(1/(y+1.2))\n",
    "    model_cat = CatBoostClassifier(iterations=200,depth=6,l2_leaf_reg=50,learning_rate=0.8,random_strength=1,\n",
    "                                      loss_function='MultiClassOneVsAll',verbose=False)\n",
    "\n",
    "    model_cat.fit(train,y,eval_set=(train_1,y_5),early_stopping_rounds=20,sample_weight = w)\n",
    "    kkk = model_cat.predict(T)\n",
    "    #kkk1 = np.where((kkk1 == 1)|(kkk == 1),1,0)\n",
    "    kkk1 = np.where((kkk == 1),kkk1+1,kkk1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk1 = kkk1.reshape(-1,1)\n",
    "cnt = model.predict(T)\n",
    "cnt = np.where(kkk1 > 0, 2, cnt)\n",
    "cnt = np.where((kkk1 <= 0)&(cnt == 2), 1, cnt)\n",
    "report = f1_score(y_validation, cnt, average=None)\n",
    "print(classification_report(y_validation, cnt, digits=4))\n",
    "Score = report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6\n",
    "print('Score: ', report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq = kkk1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train/is_train_20190710.txt')\n",
    "train = pd.merge(train, pro, how='left')\n",
    "y = train.label\n",
    "train = train.drop('label',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('train//is_train_20190730.txt')\n",
    "y_5 = train_1.label\n",
    "train_1 = pd.merge(train_1, pro, how='left')\n",
    "train_1 = train_1.drop('label',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train\n",
    "X_validation = train_1\n",
    "y_train = y\n",
    "y_validation = y_5\n",
    "w = np.array((y_train+1))\n",
    "modelk = CatBoostClassifier(iterations=200,depth=6,l2_leaf_reg=8,learning_rate=0.5,random_strength=1,\n",
    "                                      loss_function='MultiClassOneVsAll',logging_level='Verbose')\n",
    "\n",
    "modelk.fit(X_train,y_train,eval_set=(X_validation,y_validation),sample_weight=w,early_stopping_rounds=20,verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = modelk.predict(X_validation)\n",
    "ans = np.where((kkk1 == 1),2, ans)\n",
    "print(classification_report(y_5, ans, digits=4))\n",
    "print(classification_report(y_5, modelk.predict(X_validation), digits=4))\n",
    "report = f1_score(y_5, ans, average=None)\n",
    "\n",
    "Score = report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6\n",
    "print('Score: ', report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk = model.predict(train_1)\n",
    "print(classification_report(y_5, kkk, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk = model.predict(train_1)\n",
    "print(classification_report(y_5, kkk, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型结果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.where(kkk1 == 1,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train/is_train_20190704.txt')\n",
    "train = pd.merge(train, pro, how='left')\n",
    "y = train.label\n",
    "train = train.drop('label',1)\n",
    "y = np.where(y == 2, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = np.array(1/(y+4))\n",
    "model_1 = CatBoostClassifier(iterations=200,depth=6,l2_leaf_reg=8,learning_rate=0.5,random_strength=1,\n",
    "                                      loss_function='MultiClassOneVsAll',logging_level='Verbose', fold_len_multiplier  = 1.1)\n",
    "\n",
    "model_1.fit(train,y,eval_set=(train_1,y_5),sample_weight=w,early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kkk1 = model_1.predict(train_1)\n",
    "print(classification_report(y_5, kkk1, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.where((kkk1 == 1),1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.where((kkk1 == 1)|(kkk == 1),1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train/is_train_20190702.txt')\n",
    "train = pd.merge(train, pro, how='left')\n",
    "y = train.label\n",
    "train = train.drop('label',1)\n",
    "y = np.where(y == 2, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('train//is_train_20190730.txt')\n",
    "y_5 = train_1.label\n",
    "train_1 = pd.merge(train_1, pro, how='left')\n",
    "train_1 = train_1.drop('label',1)\n",
    "y_5 = np.where(y_5 == 2, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME\",n_estimators=50, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt.fit(train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = bdt.predict(train_1)\n",
    "print(classification_report(y_5, ada, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = np.where(kkk1 == 1, 1, ada.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_5, op, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk2 = y_1\n",
    "kkk2 = np.where(kkk1 <500, 0, 0)\n",
    "kkk2 = kkk1.reshape(1,-1)\n",
    "cnt_y = np.where(y_1 == 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(2,10)):\n",
    "    a = random.randint(0,2000)\n",
    "    path = 'train/is_train_2019070'+str(i)+'.txt'\n",
    "    train = pd.read_csv(path)\n",
    "    train = train.join(attr.set_index('linkid'), on='link')\n",
    "    train = pd.merge(train, pro, how='left')\n",
    "    y = train.label\n",
    "    train = train.drop('label',1)\n",
    "    y = np.where(y == 2, 1, 0)\n",
    "    \n",
    "    lgb_train =  lgb.Dataset(train, label=y)\n",
    "    lgb_eval = lgb.Dataset(train_1, label=y_5, reference=lgb_train) \n",
    "\n",
    "\n",
    "    parameters = {\n",
    "                      'task': 'train',\n",
    "                      'max_depth': 15,\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'num_leaves': 100,        # 叶子节点数\n",
    "                      'n_estimators': 50,\n",
    "                      'objective': 'binary',\n",
    "                      'metric': 'auc',\n",
    "                      'learning_rate': 0.2,\n",
    "                      'feature_fraction': 0.7, #小于 1.0, LightGBM 将会在每次迭代中随机选择部分特征.\n",
    "                      'bagging_fraction': 1,   #类似于 feature_fraction, 但是它将在不进行重采样的情况下随机选择部分数据\n",
    "                      'bagging_freq': 1,       #bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging        \n",
    "                      'lambda_l1': 0.38,\n",
    "                      \"nthread\": -1,\n",
    "                      'lambda_l2': 0.31,\n",
    "                      'cat_smooth': 10,        #用于分类特征,这可以降低噪声在分类特征中的影响, 尤其是对数据很少的类别\n",
    "                      'is_unbalance': False,   #适合二分类。这里如果设置为True，评估结果降低3个点\n",
    "                      'verbose': 0\n",
    "                      }\n",
    "\n",
    "\n",
    "    evals_result = {}  #记录训练结果所用\n",
    "    gbm_model = lgb.train(parameters,\n",
    "                        lgb_train,\n",
    "                        valid_sets=[lgb_train,lgb_eval],\n",
    "                        num_boost_round=2000,          #提升迭代的次数\n",
    "                        early_stopping_rounds=20,\n",
    "                        evals_result=evals_result,\n",
    "                        verbose_eval=10\n",
    "                        )\n",
    "\n",
    "\n",
    "    prediction = gbm_model.predict(T,num_iteration=gbm_model.best_iteration)\n",
    "    ppp = np.where(prediction>0.88,1,0)\n",
    "    kkk2 = np.where((kkk2 == 1)|(ppp == 1),1,0)\n",
    "    print(classification_report(cnt_y, ppp, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(10,30)):\n",
    "    a = random.randint(0,2000)\n",
    "    path = 'train/is_train_201907'+str(i)+'.txt'\n",
    "    train = pd.read_csv(path)\n",
    "    train = train.join(attr.set_index('linkid'), on='link')\n",
    "    train = pd.merge(train, pro, how='left')\n",
    "    y = train.label\n",
    "    train = train.drop('label',1)\n",
    "    y = np.where(y == 2, 1, -1)\n",
    "    \n",
    "    lgb_train =  lgb.Dataset(train, label=y)\n",
    "    lgb_eval = lgb.Dataset(train_1, label=y_5, reference=lgb_train) \n",
    "\n",
    "\n",
    "    parameters = {\n",
    "                      'task': 'train',\n",
    "                      'max_depth': 15,\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'num_leaves': 100,        # 叶子节点数\n",
    "                      'n_estimators': 50,\n",
    "                      'objective': 'binary',\n",
    "                      'metric': 'auc',\n",
    "                      'learning_rate': 0.2,\n",
    "                      'feature_fraction': 0.7, #小于 1.0, LightGBM 将会在每次迭代中随机选择部分特征.\n",
    "                      'bagging_fraction': 1,   #类似于 feature_fraction, 但是它将在不进行重采样的情况下随机选择部分数据\n",
    "                      'bagging_freq': 1,       #bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging        \n",
    "                      'lambda_l1': 0.38,\n",
    "                      \"nthread\": -1,\n",
    "                      'lambda_l2': 0.31,\n",
    "                      'cat_smooth': 10,        #用于分类特征,这可以降低噪声在分类特征中的影响, 尤其是对数据很少的类别\n",
    "                      'is_unbalance': False,   #适合二分类。这里如果设置为True，评估结果降低3个点\n",
    "                      'verbose': 0\n",
    "                      }\n",
    "\n",
    "\n",
    "    evals_result = {}  #记录训练结果所用\n",
    "    gbm_model = lgb.train(parameters,\n",
    "                        lgb_train,\n",
    "                        valid_sets=[lgb_train,lgb_eval],\n",
    "                        num_boost_round=200,          #提升迭代的次数\n",
    "                        early_stopping_rounds=5,\n",
    "                        evals_result=evals_result,\n",
    "                        verbose_eval=10\n",
    "                        )\n",
    "\n",
    "\n",
    "    prediction = gbm_model.predict(T,num_iteration=gbm_model.best_iteration)\n",
    "    ppp = np.where(prediction>0.88,1,0)\n",
    "    kkk2 = np.where((kkk2 == 1)|(ppp == 1),1,0)\n",
    "    print(classification_report(cnt_y, ppp, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(cnt_y, kkk2.reshape(-1,1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk1 = np.where((kkk1 == 1)|(ada == 1),1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train/is_train_20190702.txt')\n",
    "X_train = X_train.join(attr.set_index('linkid'), on='link')\n",
    "X_train = pd.merge(X_train, pro, how='left')\n",
    "y_train = X_train.label\n",
    "X_train = X_train.drop('label',1)\n",
    "\n",
    "X_validation = pd.read_csv('one_day_speed.txt')\n",
    "X_validation = X_validation.join(attr.set_index('linkid'), on='link')\n",
    "X_validation = pd.merge(X_validation, pro, how='left')\n",
    "y_validation = X_validation.label\n",
    "X_validation = X_validation.drop('label',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array((y_train+1))\n",
    "model = CatBoostClassifier(iterations=200,depth=6,l2_leaf_reg=8,learning_rate=0.5,random_strength=1,\n",
    "                                      loss_function='MultiClassOneVsAll',logging_level='Verbose')\n",
    "\n",
    "model.fit(X_train,y_train,eval_set=(X_validation,y_validation),sample_weight=w,early_stopping_rounds=20,verbose=False)\n",
    "\n",
    "report = f1_score(y_validation, model.predict(X_validation), average=None)\n",
    "print(classification_report(y_validation, model.predict(X_validation), digits=4))\n",
    "Score = report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6\n",
    "print('Score: ', report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kkk1 = kkk1.reshape(-1,1)\n",
    "cnt = model.predict(T)\n",
    "cnt = np.where(kkk1 > 0, 2, cnt)\n",
    "#cnt = np.where((kkk1 <= 1)&(cnt == 2), 1, cnt)\n",
    "report = f1_score(y_validation, cnt, average=None)\n",
    "print(classification_report(y_validation, cnt, digits=4))\n",
    "Score = report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6\n",
    "print('Score: ', report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T['label'] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anss = T[['link', 'current_slice_id', 'future_slice_id', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anss.to_csv('2020_11_29.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwe = pd.read_csv('2020_11_29.csv')\n",
    "qwe['label'] = qwe['label'] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwe.to_csv('2020_11_29_lb.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.where(kkk1 != 0, 1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
